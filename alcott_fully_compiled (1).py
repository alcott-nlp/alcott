# -*- coding: utf-8 -*-
"""Alcott Fully Compiled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mjNbyh7_SxM7FKymzOkd95JcBrr1VTK5
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
# %matplotlib inline

!wget '/content/entropy prediction data - train2.tsv'

words = open('/content/entropy prediction data - train2.tsv', 'r').read().splitlines()
words[:5]

len(words)

chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i,s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s,i in stoi.items()}
print(itos)

block_size = 3

def build_dataset(words):
  X, Y = [], []
  for w in words:

    context = [0] * block_size
    for ch in w + '.':
      ix = stoi[ch]
      X.append(context)
      Y.append(ix)
      context = context[1:] + [ix]

  X = torch.tensor(X)
  Y = torch.tensor(Y)
  print(X.shape, Y.shape)
  return X, Y

import random
random.seed(42)
random.shuffle(words)
n1 = int(0.8*len(words))
n2 = int(0.9*len(words))

Xtr, Ytr = build_dataset(words[:n1])
Xdev, Ydev = build_dataset(words[n1:n2])
Xte, Yte = build_dataset(words[n2:])

g = torch.Generator().manual_seed(2147483647)
C = torch.randn((27, 10), generator=g)
W1 = torch.randn((30, 200), generator=g)
b1 = torch.randn(200, generator=g)
W2 = torch.randn((200, 27), generator=g)
b2 = torch.randn(27, generator=g)
parameters = [C, W1, b1, W2, b2]

sum(p.nelement() for p in parameters)

for p in parameters:
  p.requires_grad = True

lre = torch.linspace(-3, 0, 1000)
lrs = 10**lre

lri = []
lossi = []
stepi = []

for i in range(200000):
  ix = torch.randint(0, Xtr.shape[0], (32,))
  emb = C[Xtr[ix]]
  h = torch.tanh(emb.view(-1, 30) @ W1 + b1)
  logits = h @ W2 + b2
  loss = F.cross_entropy(logits, Ytr[ix])
  for p in parameters:
    p.grad = None
  loss.backward()
  lr = 0.1 if i < 100000 else 0.01
  for p in parameters:
    p.data += -lr * p.grad
  stepi.append(i)
  lossi.append(loss.log10().item())

plt.plot(stepi, lossi)

emb = C[Xtr]
h = torch.tanh(emb.view(-1, 30) @ W1 + b1)
logits = h @ W2 + b2
loss = F.cross_entropy(logits, Ytr)
loss

emb = C[Xdev]
h = torch.tanh(emb.view(-1, 30) @ W1 + b1)
logits = h @ W2 + b2
loss = F.cross_entropy(logits, Ydev)
loss

emb = C[Xte]
h = torch.tanh(emb.view(-1, 30) @ W1 + b1)
logits = h @ W2 + b2
loss = F.cross_entropy(logits, Yte)
loss

if C.shape[0] > len(itos):
    for i in range(len(itos), C.shape[0]):
        itos[i] = f"unknown_{i}"


plt.figure(figsize=(8,8))
plt.scatter(C[:,0].data, C[:,1].data, s=200)
for i in range(C.shape[0]):
    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha="center", va="center", color='white')
plt.grid('minor')

g = torch.Generator().manual_seed(2147483647 + 10)

for _ in range(20):

    out = []
    context = [0] * block_size
    while True:
      emb = C[torch.tensor([context])]
      h = torch.tanh(emb.view(1, -1) @ W1 + b1)
      logits = h @ W2 + b2
      probs = F.softmax(logits, dim=1)
      ix = torch.multinomial(probs, num_samples=1, generator=g).item()
      context = context[1:] + [ix]
      out.append(ix)
      if ix == 0:
        break

    print(''.join(itos[i] for i in out))

!pip install tensorflow
import tensorflow as tf
import pandas as pd
from tensorflow.keras.layers import Conv2D

X_train = pd.read_csv("/content/Reshaped(cnn) - Sheet9.csv", header=None)
Y_train = pd.read_csv("/content/Reshaped(cnn) - Sheet10.csv", header=None)

from keras.models import Sequential
from keras.layers import Dense
from tensorflow.keras.layers import Dropout
dropout_rate = 0.2
dropout_layer = Dropout(dropout_rate)

import numpy as np

X_train_array = X_train.to_numpy()

X_train_array = X_train_array.reshape(25000, 2)

print(X_train_array.shape)

Y_train_array = Y_train.to_numpy()

Y_train_array = Y_train_array.reshape(25000, 2)

print(Y_train_array.shape)

classifier.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy')
from sklearn.preprocessing import OneHotEncoder
one_hot_encoder = OneHotEncoder(sparse=False)
one_hot_encoder.fit(Y_train)
Y_encoded = one_hot_encoder.transform(Y_train)

X_train = np.concatenate((X_train, np.zeros((X_train.shape[0], 8))), axis=1)
Y_encoded = np.concatenate((Y_encoded, np.zeros((X_train.shape[0] - Y_encoded.shape[0], Y_encoded.shape[1]))), axis=0)

!pip install keras
from keras.layers import MaxPooling2D
from keras.layers import Flatten

X_train = X_train.reshape(-1, 25000, 1, 1)
print(X_train.shape)

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(25000, 1, 1)),
    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 1), activation="relu"),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 1)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(units=32, activation="relu"),
    tf.keras.layers.Dense(units=2, activation="softmax")
])

Y_train = Y_train[:10]

model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
model.fit(X_train, Y_train, epochs=100)

import torch.nn as nn

class MyCNN(nn.Module):
    def __init__(self):
        super(MyCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 1, kernel_size=3)
        self.flatten = nn.Flatten()
        self.linear = nn.Linear(9, 1)

    def forward(self, x):
        x = torch.sigmoid(self.linear(self.flatten(self.conv1(x))))
        return x

net = MyCNN()

weights = net.linear.weight.data
bias = net.linear.bias.data

def logistic_regression(x):
    x = x.view(-1, 9)
    return torch.sigmoid(weights * x + bias)

x = torch.randn(1, 1, 3, 3)
y = logistic_regression(x)
print(y)